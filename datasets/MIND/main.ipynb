{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoConfig, AutoModelForMaskedLM\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from collections import Counter\n",
    "from matplotlib import pyplot as plt"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Zero-shot LMRecSys"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "def convert(ids, n_mask=10):\n",
    "    # Input: [1, 2, 3, 4, 5]\n",
    "    # Output: A user watched A, B, C, D, E. Now the user may want to watch {[MASK] * n_mask}.\n",
    "    s = 'A user watched '\n",
    "    for id in ids:\n",
    "        s += id2name[id] + ', '\n",
    "    s = s.strip()[:-1] + '. '\n",
    "    s += 'Now the user may want to watch '\n",
    "    s += tokenizer.mask_token * n_mask\n",
    "    s += '.'\n",
    "    return s\n",
    "\n",
    "def score(ids, gpu=False):\n",
    "    # Input: [1, 2, 3, 4, 5]\n",
    "    # Output: Prob(y | [1, 2, 3, 4, 5]), where y = [1, ..., n_movie]\n",
    "    logits_all = []\n",
    "    for n_mask in range(1, 11): # leave 1~10 masks (O(10) inferences)\n",
    "        input = tokenizer([convert(ids, n_mask=n_mask)], max_length=512, return_tensors='pt')\n",
    "        if gpu: input = {key: input[key].cuda() for key in input}\n",
    "        output = model(**input).logits # shape = (1 x seq_lenth x vocab_size)\n",
    "        mask_idxs = [i for i, token in enumerate(input['input_ids'][0]) if token == tokenizer.mask_token_id] # shape = (n_mask)\n",
    "        logits = output[0, mask_idxs].softmax(-1).log().detach().cpu().numpy() # shape = (n_mask x vocab_size)\n",
    "        logits_all.append(logits)\n",
    "    \n",
    "    id2score = [np.mean([logits_all[len(tokens) - 1][i, token] for i, token in enumerate(tokens)]) for tokens in id2tokens]\n",
    "    return id2score"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "gpu = True # Set True to use GPU for inference!\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')\n",
    "model = AutoModelForMaskedLM.from_pretrained('bert-base-cased').eval()\n",
    "if gpu: model = model.cuda()\n",
    "\n",
    "data = [json.loads(line) for line in open('data.jsonl')]\n",
    "id2name = json.load(open('id2name.json'))\n",
    "id2tokens = [tokenizer(name, add_special_tokens=False)['input_ids'][:10] for name in id2name] # max 10 tokens for each item"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "idx += 1\n",
    "ids, label = data[idx]['ids'][-7:][:-2], data[idx]['ids'][-7:][-2]\n",
    "id2score = score(ids, gpu=gpu)\n",
    "top_ids = np.array(id2score).argsort()[::-1].tolist()\n",
    "\n",
    "print(convert(ids))\n",
    "print()\n",
    "print('\\n'.join([str((id2name[id], id2score[id])) for id in top_ids[:5]]))\n",
    "print()\n",
    "print('\\n'.join([str((id2name[id], id2score[id])) for id in top_ids[-5:]]))\n",
    "print()\n",
    "print(id2name[label], top_ids.index(label))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "A user watched Opinion: Colin Kaepernick is about to get what he deserves: a chance, Actresses who've played Queen Elizabeth II, Husband of missing woman arrested, booked on murder charges, St. Mark's square closed as Venice flood waters rise, A 16-second spasm of violence leaves 2 dead at Saugus High School. Now the user may want to watch [MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK].\n",
      "\n",
      "('3..2..1...Northern Irish bar-goers celebrate same-sex marriage', -5.169982)\n",
      "(\"Who's in, who's out? Early Ohio first-round playoff projections from JoeEitel.com\", -5.27243)\n",
      "(\"'Go, go, go!': Football fans cheer for black cat invading NFL field mid-game\", -5.4991617)\n",
      "(\"Here's what it's like to live at the top of the second-tallest apartment building in the world for less than $1,400 a month\", -5.501483)\n",
      "(\"Here's what it's like to live at the top of the second-tallest apartment building in the world for less than $1,400 a month\", -5.501483)\n",
      "\n",
      "('Milwaukee Brewers 2020 arbitration estimates released', -13.064908)\n",
      "('Bengals activate Darqueze Dennard', -13.078684)\n",
      "('Nordstrom Opening NYC Flagship', -13.083083)\n",
      "('Republicans disrupt impeachment deposition', -13.218666)\n",
      "('Mississippi Bourbon Punch', -13.542472)\n",
      "\n",
      "Today in History: November 15 12007\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "def compute_recall_at_k(top_preds, labels, k=20):\n",
    "    assert(len(top_preds) == len(labels))\n",
    "    r_k = (top_preds[:, :k] == labels.reshape(-1, 1)).sum(-1).mean(-1)\n",
    "    return float(r_k)\n",
    "\n",
    "def compute_mrr_at_k(top_preds, labels, k=20):\n",
    "    assert(len(top_preds) == len(labels))\n",
    "    top_preds, labels = top_preds[:, :k].tolist(), labels.tolist()\n",
    "    mrr_k = np.mean([1 / (top_pred.index(label) + 1) if label in top_pred else 0. for top_pred, label in zip(top_preds, labels)])\n",
    "    return float(mrr_k)\n",
    "\n",
    "top_preds, labels = [], []\n",
    "for item in tqdm(data[:100]): # evaluate first 50 items\n",
    "    item['ids'] = item['ids'][-7:]\n",
    "    id2score = score(item['ids'][:-2], gpu=gpu)\n",
    "    top_pred = np.array(id2score).argsort()[::-1].tolist()\n",
    "    top_preds.append(top_pred)\n",
    "    labels.append(item['ids'][-2])\n",
    "top_preds, labels = np.array(top_preds), np.array(labels)\n",
    "\n",
    "metrics = {\n",
    "    'r@20': compute_recall_at_k(top_preds, labels),\n",
    "    'mrr@20': compute_mrr_at_k(top_preds, labels)\n",
    "}\n",
    "print(metrics)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 100/100 [03:28<00:00,  2.09s/it]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'r@20': 0.0, 'mrr@20': 0.0}\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "len(id2name)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "96700"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.1 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "interpreter": {
   "hash": "e329eaacdde1c3f16b1d348b966d5b9ad2a51ea4006f452504e74c885ba70bb5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}