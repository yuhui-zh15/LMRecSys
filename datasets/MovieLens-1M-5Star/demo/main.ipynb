{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoConfig, AutoModelForMaskedLM\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from collections import Counter\n",
    "from matplotlib import pyplot as plt"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Zero-shot LMRecSys"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "def convert(ids, n_mask=10):\n",
    "    # Input: [1, 2, 3, 4, 5]\n",
    "    # Output: A user watched A, B, C, D, E. Now the user may want to watch {[MASK] * n_mask}.\n",
    "    s = 'A user watched '\n",
    "    for id in ids:\n",
    "        s += id2name[id] + ', '\n",
    "    s = s.strip()[:-1] + '. '\n",
    "    s += 'Now the user may want to watch '\n",
    "    s += tokenizer.mask_token * n_mask\n",
    "    s += '.'\n",
    "    return s\n",
    "\n",
    "def score(ids, gpu=False):\n",
    "    # Input: [1, 2, 3, 4, 5]\n",
    "    # Output: Prob(y | [1, 2, 3, 4, 5]), where y = [1, ..., n_movie]\n",
    "    logits_all = []\n",
    "    for n_mask in range(1, 11): # leave 1~10 masks (O(10) inferences)\n",
    "        input = tokenizer([convert(ids, n_mask=n_mask)], max_length=512, return_tensors='pt')\n",
    "        if gpu: input = {key: input[key].cuda() for key in input}\n",
    "        output = model(**input).logits # shape = (1 x seq_lenth x vocab_size)\n",
    "        mask_idxs = [i for i, token in enumerate(input['input_ids'][0]) if token == tokenizer.mask_token_id] # shape = (n_mask)\n",
    "        logits = output[0, mask_idxs].softmax(-1).log().detach().cpu().numpy() # shape = (n_mask x vocab_size)\n",
    "        logits_all.append(logits)\n",
    "    \n",
    "    id2score = [np.mean([logits_all[len(tokens) - 1][i, token] for i, token in enumerate(tokens)]) for tokens in id2tokens]\n",
    "    return id2score"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "gpu = True # Set True to use GPU for inference!\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')\n",
    "model = AutoModelForMaskedLM.from_pretrained('bert-base-cased').eval()\n",
    "if gpu: model = model.cuda()\n",
    "\n",
    "data = [json.loads(line) for line in open('data.jsonl')]\n",
    "id2name = json.load(open('id2name.json'))\n",
    "id2tokens = [tokenizer(name, add_special_tokens=False)['input_ids'][:10] for name in id2name] # max 10 tokens for each item"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "idx = 0\n",
    "ids, label = data[idx]['ids'][:7][:-2], data[idx]['ids'][:7][-2]\n",
    "id2score = score(ids, gpu=gpu)\n",
    "top_ids = np.array(id2score).argsort()[::-1].tolist()\n",
    "\n",
    "print(convert(ids))\n",
    "print()\n",
    "print('\\n'.join([str((id2name[id], id2score[id])) for id in top_ids[:5]]))\n",
    "print()\n",
    "print('\\n'.join([str((id2name[id], id2score[id])) for id in top_ids[-5:]]))\n",
    "print()\n",
    "print(id2name[label], top_ids.index(label))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "A user watched Ben-Hur, Dumbo, Schindler's List, Beauty and the Beast, Toy Story. Now the user may want to watch [MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK].\n",
      "\n",
      "('If....', -5.2231994)\n",
      "('Twilight', -5.2773)\n",
      "('The Specials', -5.667444)\n",
      "('Casablanca', -5.688287)\n",
      "('Aliens', -5.6935673)\n",
      "\n",
      "('Sparrows', -14.944679)\n",
      "('Volunteers', -15.050343)\n",
      "('Endurance', -15.124055)\n",
      "('Tainted', -15.305075)\n",
      "('Lauderdale', -16.802883)\n",
      "\n",
      "A Bug's Life 155\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "def compute_recall_at_k(top_preds, labels, k=20):\n",
    "    assert(len(top_preds) == len(labels))\n",
    "    r_k = (top_preds[:, :k] == labels.reshape(-1, 1)).sum(-1).mean(-1)\n",
    "    return float(r_k)\n",
    "\n",
    "def compute_mrr_at_k(top_preds, labels, k=20):\n",
    "    assert(len(top_preds) == len(labels))\n",
    "    top_preds, labels = top_preds[:, :k].tolist(), labels.tolist()\n",
    "    mrr_k = np.mean([1 / (top_pred.index(label) + 1) if label in top_pred else 0. for top_pred, label in zip(top_preds, labels)])\n",
    "    return float(mrr_k)\n",
    "\n",
    "top_preds, labels = [], []\n",
    "for item in tqdm(data): # evaluate first 50 items\n",
    "    id2score = score(item['ids'][:-2], gpu=gpu)\n",
    "    top_pred = np.array(id2score).argsort()[::-1].tolist()\n",
    "    top_preds.append(top_pred)\n",
    "    labels.append(item['ids'][-2])\n",
    "top_preds, labels = np.array(top_preds), np.array(labels)\n",
    "\n",
    "metrics = {\n",
    "    'r@20': compute_recall_at_k(top_preds, labels),\n",
    "    'mrr@20': compute_mrr_at_k(top_preds, labels)\n",
    "}\n",
    "print(metrics)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 5337/5337 [22:45<00:00,  3.91it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'r@20': 0.011617013303353943, 'mrr@20': 0.0018845515385167014}\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.1 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "interpreter": {
   "hash": "e329eaacdde1c3f16b1d348b966d5b9ad2a51ea4006f452504e74c885ba70bb5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}