{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import logging\n",
    "import sys\n",
    "import json\n",
    "import torch\n",
    "from argparse import ArgumentParser\n",
    "import pytorch_lightning as pl\n",
    "import numpy as np\n",
    "from data import MovieLensDataLoader\n",
    "from model import GRU4Rec, MLMRecSys\n",
    "from transformers import AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "from utils import compute_metrics"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')\n",
    "model = MLMRecSys.load_from_checkpoint('wandb/run-20210722_070252-21se3gnd/files/LMRecSys/21se3gnd/checkpoints/epoch=7-step=5342.ckpt').to('cuda:0').eval()\n",
    "id2name = json.load(open('datasets/MovieLens-1M-5Star/id2name.json'))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "def convert_to_features(ids, id2name, tokenizer, n_mask=10, max_seq_length=128, input_str=None, to_tensor=True, device='cuda:0'):\n",
    "\n",
    "    def pattern(ids, id2name, mask_token, n_mask):\n",
    "        s = 'A user watched '\n",
    "        for id in ids:\n",
    "            s += id2name[id] + ', '\n",
    "        s = s.strip()[:-1] + '. '\n",
    "        s += 'Now the user may want to watch'\n",
    "        s += mask_token * n_mask\n",
    "        return s\n",
    "    \n",
    "    assert(ids is None or input_str is None)\n",
    "\n",
    "    if input_str is None:\n",
    "        input_str = pattern(ids, id2name, tokenizer.mask_token, n_mask)\n",
    "\n",
    "    result = tokenizer(\n",
    "        input_str,\n",
    "        add_special_tokens=True,\n",
    "        max_length=max_seq_length,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_attention_mask=True,\n",
    "        return_token_type_ids=True\n",
    "    )\n",
    "    result['input_str'] = input_str\n",
    "    result['mask_idxs'] = [i for i, id in enumerate(result['input_ids']) if id == tokenizer.mask_token_id]\n",
    "\n",
    "    if to_tensor: \n",
    "        for key in ['input_ids', 'token_type_ids', 'attention_mask', 'mask_idxs']:\n",
    "            result[key] = torch.tensor([result[key]]).to(device)\n",
    "\n",
    "    return result"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# Reproduce the numbers\n",
    "data = [json.loads(line) for line in open('datasets/MovieLens-1M-5Star/data.jsonl')]\n",
    "all_labels, all_top_preds = [], []\n",
    "for item in tqdm(data):\n",
    "    ids = item['ids'][:5]\n",
    "    inputs = convert_to_features(ids, id2name, tokenizer, input_str=None)\n",
    "    outputs = model(inputs)\n",
    "    logits = outputs['label_logits_aggregated']\n",
    "    top_preds = logits.argsort(-1)\n",
    "    all_top_preds.append(top_preds[0].tolist())\n",
    "    all_labels.append(item['ids'][5])\n",
    "compute_metrics(np.array(all_top_preds), np.array(all_labels), prefix='val')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 5337/5337 [01:37<00:00, 54.82it/s]\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'val/r@20': 0.12235338204984074,\n",
       " 'val/mrr@20': 0.02702487683190047,\n",
       " 'val/ndcg@20': 0.05358626384134088}"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# Test input ids\n",
    "data = [json.loads(line) for line in open('datasets/MovieLens-1M-5Star/data.jsonl')]\n",
    "\n",
    "item = data[0]\n",
    "ids = item['ids'][:5]\n",
    "inputs = convert_to_features(ids, id2name, tokenizer, input_str=None)\n",
    "outputs = model(inputs)\n",
    "logits = outputs['label_logits_aggregated']\n",
    "top_preds = logits.argsort(-1)\n",
    "\n",
    "print(inputs['input_str'])\n",
    "print([id2name[id] for id in top_preds[0][:5]])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "A user watched Ben-Hur, Dumbo, Schindler's List, Beauty and the Beast, Toy Story. Now the user may want to watch[MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK]\n",
      "['Good Will Hunting', 'Beauty and the Beast', 'Toy Story', 'Braveheart', 'The Lion King']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "# Test input strings\n",
    "input_str = input()\n",
    "inputs = convert_to_features(None, id2name, tokenizer, input_str=input_str)\n",
    "outputs = model(inputs)\n",
    "logits = outputs['label_logits_aggregated']\n",
    "top_preds = logits.argsort(-1)\n",
    "\n",
    "print(inputs['input_str'])\n",
    "display([(id2name[id], logits[0][id].item()) for id in top_preds[0][:5]])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Now the user may want to watch[MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK]\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "[('The Sixth Sense', 44.42577362060547),\n",
       " ('Pulp Fiction', 44.58979034423828),\n",
       " ('The Shawshank Redemption', 44.60279083251953),\n",
       " ('Fight Club', 44.667724609375),\n",
       " ('The Princess Bride', 44.74372100830078)]"
      ]
     },
     "metadata": {}
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "source": [
    "# MLM\n",
    "input_str = 'It is a [MASK] day.'\n",
    "\n",
    "# model = MLMRecSys().to('cuda:0').eval()\n",
    "inputs = tokenizer(input_str, return_tensors='pt')\n",
    "mask_idx = [i for i, token_id in enumerate(inputs['input_ids'][0]) if token_id == tokenizer.mask_token_id]\n",
    "assert(len(mask_idx) == 1)\n",
    "mask_idx = mask_idx[0]\n",
    "for key in inputs: inputs[key] = inputs[key].to('cuda:0')\n",
    "print(inputs, mask_idx)\n",
    "top_preds = model.model(**inputs).logits[0][mask_idx].argsort().tolist()[::-1][:10]\n",
    "tokenizer.decode(top_preds)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'input_ids': tensor([[ 101, 1135, 1110,  170,  103, 1285,  119,  102]], device='cuda:0'), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')} 4\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'. \" by long the good ; - October new'"
      ]
     },
     "metadata": {},
     "execution_count": 56
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# AmazonPantry"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "id2name = json.load(open('datasets/AmazonPantry/id2name.json'))\n",
    "model.hparams.data_dir = 'datasets/AmazonPantry/'\n",
    "model.create_verbalizers()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "source": [
    "# Reproduce the numbers\n",
    "data = [json.loads(line) for line in open('datasets/AmazonPantry/data.jsonl')]\n",
    "all_labels, all_top_preds = [], []\n",
    "for item in tqdm(data[:2000]):\n",
    "    if len(item['ids']) < 7: continue\n",
    "    ids = item['ids'][:5]\n",
    "    inputs = convert_to_features(ids, id2name, tokenizer, input_str=None, max_seq_length=512)\n",
    "    outputs = model(inputs)\n",
    "    logits = outputs['label_logits_aggregated']\n",
    "    top_preds = logits.argsort(-1)\n",
    "    all_top_preds.append(top_preds[0].tolist())\n",
    "    all_labels.append(item['ids'][5])\n",
    "compute_metrics(np.array(all_top_preds), np.array(all_labels), prefix='val')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 2000/2000 [00:17<00:00, 112.15it/s]\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'val/r@20': 0.0047562425683709865,\n",
       " 'val/mrr@20': 0.0019157088122605365,\n",
       " 'val/ndcg@20': 0.0027743726963040762}"
      ]
     },
     "metadata": {},
     "execution_count": 52
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "# Test input ids\n",
    "data = [json.loads(line) for line in open('datasets/AmazonPantry/data.jsonl')]\n",
    "\n",
    "item = data[9]\n",
    "print(item['ids'])\n",
    "ids = item['ids'][:5]\n",
    "inputs = convert_to_features(ids, id2name, tokenizer, input_str=None, max_seq_length=256)\n",
    "outputs = model(inputs)\n",
    "logits = outputs['label_logits_aggregated']\n",
    "top_preds = logits.argsort(-1)\n",
    "\n",
    "print(inputs['input_str'], ids, item['ids'][5], id2name[item['ids'][5]])\n",
    "print([id2name[id] for id in top_preds[0][:5]])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[2321, 2365, 2371, 2357, 85, 426, 426, 1482, 64, 451, 451, 3359, 2055, 754, 445, 445, 3018, 397, 397, 3047, 529, 1026, 3029, 639, 1, 1094, 1490, 2040, 2710, 151, 2285, 1410, 891]\n",
      "A user watched King Arthur Flour 100% Organic Unbleached All-Purpose Flour, 80 Ounce., Nestle Toll House DelightFulls Milk Chocolate Morsels with Caramel Filling, 9 Ounce., Nestle Toll House DelightFulls Dark Chocolate Morsels with Mint Filling, 9 Ounce., Nestle Toll House Semi-Sweet Chocolate Morsels, 24 Ounce., Lundberg Family Farms Organic Basmati Rice, California Brown, 32 Ounce.. Now the user may want to watch[MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK] [2321, 2365, 2371, 2357, 85] 426 Huy Fong Hot Chili Sauce, Sriracha, 28 oz.\n",
      "['Crazy Cups Coffee, Hot Chocolate and Irish Creme Cheesecake, 22 Count.', 'Best Foods Creamy Real Mayonnaise, Gluten Free, Kosher, 64 oz.', 'Crazy Cups Coffee, Hot Chocolate and Salted Caramel, 22 Count.', 'Crazy Cups Coffee, Tea and Hot Chocolate Variety Sampler Pack for Keurig K-Cup Brewers, 30 Count.', \"Chocolate Hazelnut Butter by Justin's, Organic Cocoa, No Stir, Gluten-free, Responsibly Sourced, 16oz Jar.\"]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.1",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.1 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "e329eaacdde1c3f16b1d348b966d5b9ad2a51ea4006f452504e74c885ba70bb5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}